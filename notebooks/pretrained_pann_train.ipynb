{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827b44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jaeheonshim/music-vibes\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jaeheonshim/music-vibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "\n",
    "from vibenet import labels\n",
    "from vibenet.dataset import FMAWaveformDataset\n",
    "from vibenet.models.teacher import PANNsMLP\n",
    "from vibenet.train_utils import compute_losses, compute_metrics\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a1345",
   "metadata": {},
   "source": [
    "Acousticness, Instrumentalness, and Liveness are **likelihoods** (e.g. a track having a 1.0 acousticness represents a high *confidence* that the track is acoustic). Thus, we use binary cross-entropy loss to optimize them.\n",
    "\n",
    "Speechiness, Danceability, Energy, and Valence are **perceptual measures** (e.g. a danceability value of 0.0.0 is least danceable and 1.0 is most danceable). We use Huber and MSE loss to optimize Speechiness and Danceability, and concordance correlation coefficient (CCC) to optimize energy and valence (https://arxiv.org/abs/2003.10724)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4dd78",
   "metadata": {},
   "source": [
    "### Load the dataset and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3880cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Subset(FMAWaveformDataset('data/preprocessed/waveforms_train'), range(1024))\n",
    "test_ds = FMAWaveformDataset('data/preprocessed/waveforms_val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a2906",
   "metadata": {},
   "source": [
    "### Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8883301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PANNsMLP()\n",
    "model = model.to(device)\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6716e4c",
   "metadata": {},
   "source": [
    "### Train/Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b5051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def train():\n",
    "    global global_step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    with tqdm(train_dl, desc='Training') as pbar:\n",
    "        for data, label in pbar:\n",
    "            data, label = data.to(device).float(), label.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                pred = model(data)\n",
    "                \n",
    "                loss_total = 0.0\n",
    "                losses = compute_losses(pred, label)\n",
    "                for k, l in losses.items():\n",
    "                    writer.add_scalar(f\"train/loss/{k}\", l, global_step)\n",
    "                    loss_total += l\n",
    "                \n",
    "            scaler.scale(loss_total).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss_total.item())\n",
    "            \n",
    "            mean_loss = np.mean(train_losses)\n",
    "            writer.add_scalar(f\"train/loss\", mean_loss, global_step)\n",
    "            pbar.set_postfix({'loss': f\"{mean_loss:.4f}\"})\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "def validate():\n",
    "    global global_step\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = defaultdict(list)\n",
    "    targets = []\n",
    "\n",
    "    with tqdm(test_dl, desc='Validation') as pbar:\n",
    "        with torch.inference_mode():\n",
    "            for data, label in pbar:\n",
    "                data, label = data.to(device).float(), label.to(device).float()\n",
    "\n",
    "                pred = model(data)\n",
    "                for l, p in pred.items():\n",
    "                    preds[l].append(p.detach().cpu())\n",
    "\n",
    "                targets.append(label.detach().cpu())\n",
    "\n",
    "    preds = {k: torch.cat(preds[k], dim=0) for k in preds.keys()}\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    losses = compute_losses(preds, targets)\n",
    "    loss_total = 0.0\n",
    "            \n",
    "    for k,l in losses.items():\n",
    "        loss_total += l\n",
    "        \n",
    "    writer.add_scalar(f\"eval/loss\", loss_total, global_step)\n",
    "    \n",
    "    metrics = compute_metrics(preds, targets)\n",
    "\n",
    "    return metrics, loss_total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7096d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:11<00:00,  1.43s/it, loss=4.1137]\n",
      "Validation:  52%|█████▏    | 234/451 [01:24<01:08,  3.18it/s]"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}:\")\n",
    "    \n",
    "    writer.add_scalar(f\"epoch\", epoch, global_step)\n",
    "\n",
    "    train()\n",
    "    metrics, loss_total = validate()\n",
    "\n",
    "    print(f\"Loss: {loss_total:.4f}\")\n",
    "    for task, stats in metrics.items():\n",
    "        print(f\"\\n[{task}]\")\n",
    "        max_name_len = max(len(k) for k in stats.keys())\n",
    "        for name, val in stats.items():\n",
    "            writer.add_scalar(f\"eval/{task}/{name}\", val, global_step)\n",
    "            \n",
    "            if isinstance(val, float):\n",
    "                print(f\"  {name:<{max_name_len}} : {val:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name:<{max_name_len}} : {val}\")\n",
    "\n",
    "    if loss_total < best_loss:\n",
    "        best_loss = loss_total\n",
    "        torch.save({'state_dict': model.state_dict()},\n",
    "                   'checkpoints/pretrained_PANN_best.pt')\n",
    "        print('Saved new best model')\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    global_step += 1\n",
    "    \n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
