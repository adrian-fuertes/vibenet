{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827b44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jaeheonshim/music-vibes\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jaeheonshim/music-vibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "\n",
    "from vibenet import labels\n",
    "from vibenet.dataset import FMAWaveformDataset\n",
    "from vibenet.models.teacher import PANNsMLP\n",
    "from vibenet.train_utils import compute_losses, compute_metrics\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a1345",
   "metadata": {},
   "source": [
    "Acousticness, Instrumentalness, and Liveness are **likelihoods** (e.g. a track having a 1.0 acousticness represents a high *confidence* that the track is acoustic). Thus, we use binary cross-entropy loss to optimize them.\n",
    "\n",
    "Speechiness, Danceability, Energy, and Valence are **perceptual measures** (e.g. a danceability value of 0.0.0 is least danceable and 1.0 is most danceable). We use Huber and MSE loss to optimize Speechiness and Danceability, and concordance correlation coefficient (CCC) to optimize energy and valence (https://arxiv.org/abs/2003.10724)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4dd78",
   "metadata": {},
   "source": [
    "### Load the dataset and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FMAWaveformDataset('data/preprocessed/waveforms_train')\n",
    "test_ds = FMAWaveformDataset('data/preprocessed/waveforms_val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a2906",
   "metadata": {},
   "source": [
    "### Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8883301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PANNsMLP()\n",
    "model = model.to(device)\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6716e4c",
   "metadata": {},
   "source": [
    "### Train/Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b5051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def train():\n",
    "    global global_step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    with tqdm(train_dl, desc='Training') as pbar:\n",
    "        for data, label in pbar:\n",
    "            data, label = data.to(device).float(), label.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                pred = model(data)\n",
    "                \n",
    "                loss_total = 0.0\n",
    "                losses = compute_losses(pred, label)\n",
    "                for k, l in losses.items():\n",
    "                    writer.add_scalar(f\"train/loss/{k}\", l, global_step)\n",
    "                    loss_total += l\n",
    "                \n",
    "            scaler.scale(loss_total).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss_total.item())\n",
    "            \n",
    "            mean_loss = np.mean(train_losses)\n",
    "            writer.add_scalar(f\"train/loss\", mean_loss, global_step)\n",
    "            pbar.set_postfix({'loss': f\"{mean_loss:.4f}\"})\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "def validate():\n",
    "    global global_step\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = defaultdict(list)\n",
    "    targets = []\n",
    "\n",
    "    with tqdm(test_dl, desc='Validation') as pbar:\n",
    "        with torch.inference_mode():\n",
    "            for data, label in pbar:\n",
    "                data, label = data.to(device).float(), label.to(device).float()\n",
    "\n",
    "                pred = model(data)\n",
    "                for l, p in pred.items():\n",
    "                    preds[l].append(p.detach().cpu())\n",
    "\n",
    "                targets.append(label.detach().cpu())\n",
    "\n",
    "    preds = {k: torch.cat(preds[k], dim=0) for k in preds.keys()}\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    losses = compute_losses(preds, targets)\n",
    "    loss_total = 0.0\n",
    "            \n",
    "    for k,l in losses.items():\n",
    "        loss_total += l\n",
    "        \n",
    "    writer.add_scalar(f\"eval/loss\", loss_total, global_step)\n",
    "    \n",
    "    metrics = compute_metrics(preds, targets)\n",
    "\n",
    "    return metrics, loss_total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7096d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it, loss=3.9124]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3906\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.6125\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0172\n",
      "  rmse    : 0.1310\n",
      "  mae     : 0.1111\n",
      "  pearson : 0.8084\n",
      "  ccc     : 0.6809\n",
      "  r2      : 0.5013\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0622\n",
      "  rmse    : 0.2493\n",
      "  mae     : 0.2077\n",
      "  pearson : 0.7795\n",
      "  ccc     : 0.7614\n",
      "  r2      : 0.4039\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.6290\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4195\n",
      "\n",
      "[speechiness]\n",
      "  mse     : 0.0164\n",
      "  rmse    : 0.1282\n",
      "  mae     : 0.0981\n",
      "  pearson : -0.3925\n",
      "  ccc     : -0.1727\n",
      "  r2      : -7.2803\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0929\n",
      "  rmse    : 0.3048\n",
      "  mae     : 0.2364\n",
      "  pearson : 0.5733\n",
      "  ccc     : 0.5656\n",
      "  r2      : 0.0672\n",
      "Saved new best model\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:06<00:00,  1.33it/s, loss=2.7092]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5135\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.5945\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0202\n",
      "  rmse    : 0.1420\n",
      "  mae     : 0.1122\n",
      "  pearson : 0.7979\n",
      "  ccc     : 0.6800\n",
      "  r2      : 0.4136\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0680\n",
      "  rmse    : 0.2607\n",
      "  mae     : 0.2015\n",
      "  pearson : 0.7640\n",
      "  ccc     : 0.7151\n",
      "  r2      : 0.3481\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.5887\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4434\n",
      "\n",
      "[speechiness]\n",
      "  mse     : 0.0384\n",
      "  rmse    : 0.1958\n",
      "  mae     : 0.1802\n",
      "  pearson : -0.0499\n",
      "  ccc     : -0.0072\n",
      "  r2      : -18.3087\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0901\n",
      "  rmse    : 0.3002\n",
      "  mae     : 0.2468\n",
      "  pearson : 0.6010\n",
      "  ccc     : 0.5091\n",
      "  r2      : 0.0957\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:04<00:00,  1.61it/s, loss=2.4631]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3369\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.5330\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.1083\n",
      "  rmse    : 0.3292\n",
      "  mae     : 0.3067\n",
      "  pearson : 0.7815\n",
      "  ccc     : 0.3378\n",
      "  r2      : -2.1494\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0561\n",
      "  rmse    : 0.2368\n",
      "  mae     : 0.1904\n",
      "  pearson : 0.7881\n",
      "  ccc     : 0.7480\n",
      "  r2      : 0.4625\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.5452\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4435\n",
      "\n",
      "[speechiness]\n",
      "  mse     : 0.0030\n",
      "  rmse    : 0.0549\n",
      "  mae     : 0.0418\n",
      "  pearson : -0.0016\n",
      "  ccc     : -0.0015\n",
      "  r2      : -0.5156\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0759\n",
      "  rmse    : 0.2755\n",
      "  mae     : 0.2226\n",
      "  pearson : 0.5953\n",
      "  ccc     : 0.5526\n",
      "  r2      : 0.2383\n",
      "Saved new best model\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:05<00:00,  1.58it/s, loss=2.3567]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0840\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.4159\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0789\n",
      "  rmse    : 0.2808\n",
      "  mae     : 0.2571\n",
      "  pearson : 0.7632\n",
      "  ccc     : 0.3389\n",
      "  r2      : -1.2926\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0456\n",
      "  rmse    : 0.2136\n",
      "  mae     : 0.1706\n",
      "  pearson : 0.7896\n",
      "  ccc     : 0.7847\n",
      "  r2      : 0.5624\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.5353\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4378\n",
      "\n",
      "[speechiness]\n",
      "  mse     : 0.0043\n",
      "  rmse    : 0.0659\n",
      "  mae     : 0.0556\n",
      "  pearson : 0.2776\n",
      "  ccc     : 0.2267\n",
      "  r2      : -1.1845\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0689\n",
      "  rmse    : 0.2626\n",
      "  mae     : 0.1958\n",
      "  pearson : 0.6282\n",
      "  ccc     : 0.6100\n",
      "  r2      : 0.3080\n",
      "Saved new best model\n",
      "\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:01<?, ?it/s]\n",
      "Exception in thread Thread-14 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jaeheonshim/music-vibes/venv/lib64/python3.11/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jaeheonshim/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/jaeheonshim/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jaeheonshim/music-vibes/venv/lib64/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/multiprocessing/connection.py\", line 523, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/multiprocessing/connection.py\", line 651, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m writer.add_scalar(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m, epoch, global_step)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m metrics, loss_total = validate()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m train_losses = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_dl, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1290\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.11/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}:\")\n",
    "    \n",
    "    writer.add_scalar(f\"epoch\", epoch, global_step)\n",
    "\n",
    "    train()\n",
    "    metrics, loss_total = validate()\n",
    "\n",
    "    print(f\"Loss: {loss_total:.4f}\")\n",
    "    for task, stats in metrics.items():\n",
    "        print(f\"\\n[{task}]\")\n",
    "        max_name_len = max(len(k) for k in stats.keys())\n",
    "        for name, val in stats.items():\n",
    "            writer.add_scalar(f\"eval/{task}/{name}\", val, global_step)\n",
    "            \n",
    "            if isinstance(val, float):\n",
    "                print(f\"  {name:<{max_name_len}} : {val:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name:<{max_name_len}} : {val}\")\n",
    "\n",
    "    if loss_total < best_loss:\n",
    "        best_loss = loss_total\n",
    "        torch.save({'state_dict': model.state_dict()},\n",
    "                   'checkpoints/pretrained_PANN_best.pt')\n",
    "        print('Saved new best model')\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    global_step += 1\n",
    "    \n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
