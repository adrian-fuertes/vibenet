{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827b44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jaeheonshim/music-vibes\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jaeheonshim/music-vibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "\n",
    "from vibenet import labels\n",
    "from vibenet.dataset import FMAWaveformDataset\n",
    "from vibenet.models.teacher import PANNsMLP\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a1345",
   "metadata": {},
   "source": [
    "Acousticness, Instrumentalness, and Liveness are **likelihoods** (e.g. a track having a 1.0 acousticness represents a high *confidence* that the track is acoustic). Thus, we use binary cross-entropy loss to optimize them.\n",
    "\n",
    "Speechiness, Danceability, Energy, and Valence are **perceptual measures** (e.g. a danceability value of 0.0.0 is least danceable and 1.0 is most danceable). We use Huber and MSE loss to optimize Speechiness and Danceability, and concordance correlation coefficient (CCC) to optimize energy and valence (https://arxiv.org/abs/2003.10724)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb32b9e",
   "metadata": {},
   "source": [
    "### Define loss and validation metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7defdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(x, y, eps=1e-8):\n",
    "    x = x - x.mean()\n",
    "    y = y - y.mean()\n",
    "    return (x*y).mean() / (x.std(unbiased=False)*y.std(unbiased=False) + eps)\n",
    "\n",
    "def ccc(x, y, eps=1e-8):\n",
    "    mx, my = x.mean(), y.mean()\n",
    "    vx, vy = x.var(unbiased=False), y.var(unbiased=False)\n",
    "    cov = ((x-mx)*(y-my)).mean()\n",
    "    return (2*cov) / (vx + vy + (mx-my).pow(2) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4dd78",
   "metadata": {},
   "source": [
    "### Load the dataset and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3880cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FMAWaveformDataset('data/preprocessed/waveforms_train')\n",
    "test_ds = FMAWaveformDataset('data/preprocessed/waveforms_val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc28ec",
   "metadata": {},
   "source": [
    "### Initialize loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ef606",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber = nn.SmoothL1Loss(beta=0.2)\n",
    "mse = nn.MSELoss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Compute losses for training\n",
    "def compute_losses(pred, label):\n",
    "    return {\n",
    "        'acousticness_bce': bce(pred['acousticness'], label[:,0]),\n",
    "        'liveness_bce': bce(pred['liveness'], label[:,4]),\n",
    "        'instrumentalness_bce': bce(pred['instrumentalness'], label[:,3]),\n",
    "        'speechiness_huber': huber(pred['speechiness'], label[:,5]),\n",
    "        'danceability_mse': mse(pred['danceability'], label[:,1]),\n",
    "        'energy_ccc_loss': 1 - ccc(pred['energy'], label[:,2]),\n",
    "        'valence_ccc_loss': 1 - ccc(pred['valence'], label[:,6]),\n",
    "    }\n",
    "    \n",
    "LIKELIHOODS = {'acousticness','liveness','instrumentalness'}\n",
    "CONTINUOUS  = {'speechiness', 'danceability','energy','valence'}    \n",
    "\n",
    "def compute_metrics(pred, label):\n",
    "    out = {}\n",
    "    for i, name in enumerate(labels):\n",
    "        y = label[:, i]\n",
    "        yhat = pred[name].squeeze(-1) if pred[name].ndim > 1 else pred[name]\n",
    "        m = {}\n",
    "        \n",
    "        if name in LIKELIHOODS:\n",
    "            m['logloss'] = bce(yhat, y)\n",
    "        else:\n",
    "            mse_v = F.mse_loss(yhat, y)\n",
    "            mae_v = torch.mean(torch.abs(yhat - y))\n",
    "            m['mse'] = mse_v\n",
    "            m['rmse'] = torch.sqrt(mse_v)\n",
    "            m['mae'] = mae_v\n",
    "            m['pearson'] = pearsonr(yhat, y)\n",
    "            m['ccc'] = ccc(yhat, y)\n",
    "            var_y = y.var(unbiased=False) + 1e-8\n",
    "            m['r2'] = 1.0 - (mse_v / var_y)\n",
    "\n",
    "        out[name] = {k: (v.item() if torch.is_tensor(v) else float(v)) for k, v in m.items()}\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a2906",
   "metadata": {},
   "source": [
    "### Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8883301c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Cnn14_mAP=0.431.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mPANNsMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model = model.to(device)\n\u001b[32m      4\u001b[39m NUM_EPOCHS = \u001b[32m25\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/vibenet/models/teacher.py:15\u001b[39m, in \u001b[36mPANNsMLP.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28msuper\u001b[39m(PANNsMLP, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCnn14_mAP=0.431.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m.pann = pann_models.Cnn14(\u001b[32m32000\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m320\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m14000\u001b[39m, \u001b[32m527\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.pann.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m], strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/music-vibes/venv/lib64/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Cnn14_mAP=0.431.pth'"
     ]
    }
   ],
   "source": [
    "model = PANNsMLP()\n",
    "model = model.to(device)\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6716e4c",
   "metadata": {},
   "source": [
    "### Train/Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def train():\n",
    "    global global_step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    with tqdm(train_dl, desc='Training') as pbar:\n",
    "        for data, label in pbar:\n",
    "            data, label = data.to(device).float(), label.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                pred = model(data)\n",
    "                \n",
    "                loss_total = 0.0\n",
    "                losses = compute_losses(pred, label)\n",
    "                for k, l in losses.items():\n",
    "                    writer.add_scalar(f\"train/loss/{k}\", l, global_step)\n",
    "                    loss_total += l\n",
    "                \n",
    "            scaler.scale(loss_total).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss_total.item())\n",
    "            \n",
    "            mean_loss = np.mean(train_losses)\n",
    "            writer.add_scalar(f\"train/loss\", mean_loss, global_step)\n",
    "            pbar.set_postfix({'loss': f\"{mean_loss:.4f}\"})\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "def validate():\n",
    "    global global_step\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    preds = defaultdict(list)\n",
    "    targets = []\n",
    "\n",
    "    with tqdm(test_dl, desc='Validation') as pbar:\n",
    "        with torch.inference_mode():\n",
    "            for data, label in pbar:\n",
    "                data, label = data.to(device).float(), label.to(device).float()\n",
    "\n",
    "                pred = model(data)\n",
    "                for l, p in pred.items():\n",
    "                    preds[l].append(p.detach().cpu())\n",
    "\n",
    "                targets.append(label.detach().cpu())\n",
    "\n",
    "    preds = {k: torch.cat(preds[k], dim=0) for k in preds.keys()}\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    losses = compute_losses(preds, targets)\n",
    "    loss_total = 0.0\n",
    "            \n",
    "    for k,l in losses.items():\n",
    "        loss_total += l\n",
    "        \n",
    "    writer.add_scalar(f\"eval/loss\", loss_total, global_step)\n",
    "    \n",
    "    metrics = compute_metrics(preds, targets)\n",
    "\n",
    "    return metrics, loss_total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7096d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s, loss=3.9243]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7351\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.6202\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0316\n",
      "  rmse    : 0.1779\n",
      "  mae     : 0.1364\n",
      "  pearson : 0.5124\n",
      "  ccc     : 0.3944\n",
      "  r2      : 0.0805\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0825\n",
      "  rmse    : 0.2873\n",
      "  mae     : 0.2274\n",
      "  pearson : 0.7282\n",
      "  ccc     : 0.6703\n",
      "  r2      : 0.2086\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.7366\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4188\n",
      "\n",
      "[speechiness]\n",
      "  logloss : 0.6892\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.1592\n",
      "  rmse    : 0.3990\n",
      "  mae     : 0.3531\n",
      "  pearson : 0.7234\n",
      "  ccc     : 0.4249\n",
      "  r2      : -0.5980\n",
      "Saved new best model\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:06<00:00,  1.24it/s, loss=2.7313]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1539\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.5686\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0129\n",
      "  rmse    : 0.1134\n",
      "  mae     : 0.0979\n",
      "  pearson : 0.8064\n",
      "  ccc     : 0.7377\n",
      "  r2      : 0.6263\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0479\n",
      "  rmse    : 0.2188\n",
      "  mae     : 0.1625\n",
      "  pearson : 0.7958\n",
      "  ccc     : 0.7867\n",
      "  r2      : 0.5408\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.6037\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4285\n",
      "\n",
      "[speechiness]\n",
      "  logloss : 0.7071\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0591\n",
      "  rmse    : 0.2430\n",
      "  mae     : 0.1910\n",
      "  pearson : 0.6978\n",
      "  ccc     : 0.6904\n",
      "  r2      : 0.4071\n",
      "Saved new best model\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:05<00:00,  1.37it/s, loss=2.4648]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2083\n",
      "\n",
      "[acousticness]\n",
      "  logloss : 0.5124\n",
      "\n",
      "[danceability]\n",
      "  mse     : 0.0286\n",
      "  rmse    : 0.1691\n",
      "  mae     : 0.1351\n",
      "  pearson : 0.8380\n",
      "  ccc     : 0.5606\n",
      "  r2      : 0.1687\n",
      "\n",
      "[energy]\n",
      "  mse     : 0.0651\n",
      "  rmse    : 0.2552\n",
      "  mae     : 0.1856\n",
      "  pearson : 0.7284\n",
      "  ccc     : 0.7027\n",
      "  r2      : 0.3755\n",
      "\n",
      "[instrumentalness]\n",
      "  logloss : 0.5520\n",
      "\n",
      "[liveness]\n",
      "  logloss : 0.4246\n",
      "\n",
      "[speechiness]\n",
      "  logloss : 0.7729\n",
      "\n",
      "[valence]\n",
      "  mse     : 0.0682\n",
      "  rmse    : 0.2612\n",
      "  mae     : 0.1996\n",
      "  pearson : 0.6552\n",
      "  ccc     : 0.6547\n",
      "  r2      : 0.3150\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:05<00:00,  1.42it/s, loss=2.3891]\n",
      "Validation:   0%|          | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m writer.add_scalar(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m, epoch, global_step)\n\u001b[32m     10\u001b[39m train()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m metrics, loss_total = \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task, stats \u001b[38;5;129;01min\u001b[39;00m metrics.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m             pred = model(data)\n\u001b[32m     54\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m l, p \u001b[38;5;129;01min\u001b[39;00m pred.items():\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m                 preds[l].append(\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     57\u001b[39m             targets.append(label.detach().cpu())\n\u001b[32m     59\u001b[39m preds = {k: torch.cat(preds[k], dim=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m preds.keys()}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}:\")\n",
    "    \n",
    "    writer.add_scalar(f\"epoch\", epoch, global_step)\n",
    "\n",
    "    train()\n",
    "    metrics, loss_total = validate()\n",
    "\n",
    "    print(f\"Loss: {loss_total:.4f}\")\n",
    "    for task, stats in metrics.items():\n",
    "        print(f\"\\n[{task}]\")\n",
    "        max_name_len = max(len(k) for k in stats.keys())\n",
    "        for name, val in stats.items():\n",
    "            writer.add_scalar(f\"eval/{task}/{name}\", val, global_step)\n",
    "            \n",
    "            if isinstance(val, float):\n",
    "                print(f\"  {name:<{max_name_len}} : {val:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name:<{max_name_len}} : {val}\")\n",
    "\n",
    "    if loss_total < best_loss:\n",
    "        best_loss = loss_total\n",
    "        torch.save({'state_dict': model.state_dict()},\n",
    "                   'checkpoints/pretrained_PANN_best.pt')\n",
    "        print('Saved new best model')\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    global_step += 1\n",
    "    \n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
